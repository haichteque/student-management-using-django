import ast
import os
import json

# Configuration
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
APP_NAME = "main_app"
TARGET_FILES = [
    "hod_views.py",
    "staff_views.py",
    "student_views.py",
    "views.py"
]
CONTEXT_FILES = [
    "models.py",
    "forms.py"
]
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "tests", "generated")

def read_file(file_path):
    """Reads the content of a file."""
    try:
        with open(file_path, "r") as f:
            return f.read()
    except FileNotFoundError:
        print(f"Warning: File not found: {file_path}")
        return ""

def get_context_content():
    """Reads models and forms to provide context to the LLM."""
    context = ""
    for filename in CONTEXT_FILES:
        path = os.path.join(PROJECT_ROOT, APP_NAME, filename)
        content = read_file(path)
        context += f"\n\n--- {filename} ---\n{content}\n"
    return context

def extract_functions(file_path):
    """Parses a Python file and extracts function source codes."""
    source = read_file(file_path)
    if not source:
        return []
    
    tree = ast.parse(source)
    functions = []
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            # Get the source segment for the function
            func_source = ast.get_source_segment(source, node)
            functions.append({
                "name": node.name,
                "code": func_source
            })
    return functions

def generate_prompt(func_data, context, file_name):
    """Constructs the prompt for the LLM."""
    prompt = f"""
You are an expert Python QA engineer specializing in Django.
Your task is to write a comprehensive unit test using `pytest` for the following Django view function.

**Goal:** Achieve 100% decision coverage. You must test every `if`, `elif`, `else` branch and every `try`, `except` block.

**Constraints & Requirements:**
1.  **Mocking**: Do NOT use a real database. You MUST mock all database interactions (Django ORM calls like `.objects.get`, `.save()`, etc.) and external dependencies (like `requests`). Use `unittest.mock` or `pytest-mock`.
2.  **Framework**: Use `pytest`.
3.  **Context**: The function belongs to the file `{file_name}`.
4.  **Output**: Return ONLY the Python code for the test. Do not include markdown formatting or explanations.

**Context (Models and Forms):**
{context}

**Function to Test:**
```python
{func_data['code']}
```
"""
    return prompt

def call_llm_api(prompt):
    """
    Placeholder for the actual LLM API call.
    Replace this with your actual API client code (e.g., OpenAI, Anthropic, Google).
    """
    # Example structure for OpenAI:
    # import openai
    # response = openai.ChatCompletion.create(
    #     model="gpt-4",
    #     messages=[{"role": "user", "content": prompt}]
    # )
    # return response.choices[0].message.content
    
    print(f"    [Mock] Generating test for prompt length: {len(prompt)} chars...")
    return f"""
# Generated Test Placeholder
import pytest
from unittest.mock import MagicMock, patch
from django.urls import reverse

@pytest.mark.django_db
def test_placeholder():
    # This is a placeholder. 
    # Real implementation would be generated by the LLM based on the prompt.
    pass
"""

def main():
    # Ensure output directory exists
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # 1. Get Context
    print("Reading context (models, forms)...")
    context = get_context_content()
    
    # 2. Process each target file
    for filename in TARGET_FILES:
        file_path = os.path.join(PROJECT_ROOT, APP_NAME, filename)
        print(f"Processing {filename}...")
        
        functions = extract_functions(file_path)
        
        # Prepare output file
        test_filename = f"test_{filename}"
        test_file_path = os.path.join(OUTPUT_DIR, test_filename)
        
        with open(test_file_path, "w") as f:
            f.write(f"# Auto-generated tests for {filename}\n")
            f.write("import pytest\n")
            f.write("from unittest.mock import MagicMock, patch\n")
            f.write("from django.urls import reverse\n\n")
            
            for func in functions:
                print(f"  Generating test for function: {func['name']}")
                prompt = generate_prompt(func, context, filename)
                
                # 3. Call LLM
                test_code = call_llm_api(prompt)
                
                # 4. Save to file
                f.write(f"\n# Tests for {func['name']}\n")
                f.write(test_code)
                f.write("\n" + "-"*80 + "\n")
                
    print(f"\nDone! Generated tests are in {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
